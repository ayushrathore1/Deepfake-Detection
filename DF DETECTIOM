{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushrathore1/Deepfake-Detection/blob/main/DF%20DETECTIOM\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KXSdXooQDUUF",
        "outputId": "5388fca8-97e6-4457-bc4d-5d64b3abf650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: fer 25.10.3\n",
            "Uninstalling fer-25.10.3:\n",
            "  Successfully uninstalled fer-25.10.3\n",
            "Collecting git+https://github.com/justinshenk/fer\n",
            "  Cloning https://github.com/justinshenk/fer to /tmp/pip-req-build-mov8g420\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/justinshenk/fer /tmp/pip-req-build-mov8g420\n",
            "  Resolved https://github.com/justinshenk/fer to commit 81e83fe968caddc2b456471160e3f96c4881ed35\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (4.12.0.88)\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (2.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (2.32.4)\n",
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (2.5.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (4.67.1)\n",
            "Requirement already satisfied: moviepy<2.0,>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (1.0.3)\n",
            "Requirement already satisfied: ffmpeg-python>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (0.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fer==25.10.3) (11.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python>=0.2.0->fer==25.10.3) (1.0.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy<2.0,>=1.0.3->fer==25.10.3) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy<2.0,>=1.0.3->fer==25.10.3) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from moviepy<2.0,>=1.0.3->fer==25.10.3) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy<2.0,>=1.0.3->fer==25.10.3) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy<2.0,>=1.0.3->fer==25.10.3) (0.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->fer==25.10.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->fer==25.10.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->fer==25.10.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->fer==25.10.3) (2025.10.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer==25.10.3) (0.5.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch->fer==25.10.3) (0.23.0+cu126)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer==25.10.3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer==25.10.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer==25.10.3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer==25.10.3) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer==25.10.3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer==25.10.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->fer==25.10.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->fer==25.10.3) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->fer==25.10.3) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.0.0->fer==25.10.3) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.0.0->fer==25.10.3) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.0.0->fer==25.10.3) (0.17.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.0.0->fer==25.10.3) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.0.0->fer==25.10.3) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.0.0->fer==25.10.3) (3.1.3)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->facenet-pytorch->fer==25.10.3) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.0.0->fer==25.10.3) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.0.0->fer==25.10.3) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.0.0->fer==25.10.3) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.0.0->fer==25.10.3) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision->facenet-pytorch->fer==25.10.3) (1.3.0)\n",
            "Building wheels for collected packages: fer\n",
            "  Building wheel for fer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fer: filename=fer-25.10.3-py3-none-any.whl size=816221 sha256=da021bdb91c6bc9cae6e7c29c669fbcf5c84b90cced1de322b41d1ab4dbf9445\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-erbn_peu/wheels/ef/e4/f1/732fba0b719fb06847fae454bba3cf2d95a43a631d38738588\n",
            "Successfully built fer\n",
            "Installing collected packages: fer\n",
            "Successfully installed fer-25.10.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fer"
                ]
              },
              "id": "8df46bb6f7214dacbea7e0aeafb0461b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All packages installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Updated: Reliable installation and import of all required packages, including FER\n",
        "\n",
        "# Clean install fer from GitHub (handles ImportError issues in Colab)\n",
        "!pip install -q transformers torchaudio librosa facenet-pytorch opencv-python-headless scipy scikit-learn matplotlib seaborn tqdm\n",
        "!pip uninstall -y fer\n",
        "!pip install --no-cache-dir git+https://github.com/justinshenk/fer\n",
        "\n",
        "# Import all libraries\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "import cv2\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial.distance import cosine\n",
        "from torchvision import transforms\n",
        "from facenet_pytorch import MTCNN\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
        "from google.colab import files\n",
        "\n",
        "# Robust import for FER with fallback\n",
        "try:\n",
        "    from fer import FER\n",
        "except ImportError:\n",
        "    from fer.fer import FER  # fallback if import structure changed\n",
        "\n",
        "print(\"‚úÖ All packages installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "8s0nimIfD19x",
        "outputId": "01c6dbc7-c0c2-45ee-86d1-2a7c22d041a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Please upload FakeAVCeleb.zip...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-87a3a1ed-1ab9-420d-8311-8e2ff5c320e1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-87a3a1ed-1ab9-420d-8311-8e2ff5c320e1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving New FakeAVCeleb.zip to New FakeAVCeleb.zip\n",
            "üì¶ Extracting files...\n",
            "‚úÖ Dataset extracted successfully!\n",
            "‚ö† Warning: Folder A not found!\n",
            "‚ö† Warning: Folder B not found!\n",
            "‚ö† Warning: Folder C not found!\n",
            "‚ö† Warning: Folder D not found!\n"
          ]
        }
      ],
      "source": [
        "# Ensure required imports\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "base_path = \"FakeAVCeleb\"\n",
        "video_folders = [\n",
        "    \"FakeVideo-FakeAudio\",\n",
        "    \"FakeVideo-RealAudio\",\n",
        "    \"RealVideo-FakeAudio\",\n",
        "    \"RealVideo-RealAudio\"\n",
        "]\n",
        "label_map = {\n",
        "    \"RealVideo-RealAudio\": \"real\",\n",
        "    \"FakeVideo-FakeAudio\": \"fake\",\n",
        "    \"FakeVideo-RealAudio\": \"fake\",\n",
        "    \"RealVideo-FakeAudio\": \"fake\"\n",
        "}\n",
        "\n",
        "# Verify dataset structure\n",
        "for folder in video_folders:\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        video_count = len([f for f in os.listdir(folder_path) if f.endswith('.mp4')])\n",
        "        print(f\"‚úì Folder {folder} ({label_map[folder]}): {video_count} videos\")\n",
        "    else:\n",
        "        print(f\"‚ö† Warning: Folder {folder} not found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u6Z9mxTE3g2"
      },
      "outputs": [],
      "source": [
        "# Load models with error handling\n",
        "print(\"üîÑ Loading models...\")\n",
        "\n",
        "# Audio emotion model\n",
        "try:\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
        "        \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
        "    )\n",
        "    aem_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "        \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
        "    )\n",
        "    aem_model.eval()\n",
        "    print(\"‚úÖ Audio emotion model loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading audio model: {e}\")\n",
        "\n",
        "# Face detection model\n",
        "try:\n",
        "    mtcnn = MTCNN(keep_all=False, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"‚úÖ Face detection model loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading face detection: {e}\")\n",
        "\n",
        "# Visual emotion recognition (using FER - real emotion detection)\n",
        "try:\n",
        "    emotion_detector = FER(mtcnn=True)\n",
        "    print(\"‚úÖ Visual emotion detection model loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading emotion detector: {e}\")\n",
        "\n",
        "# Configuration parameters\n",
        "CONFIG = {\n",
        "    'frame_sampling_rate': 10,  # Extract every Nth frame\n",
        "    'max_frames': 30,  # Maximum frames to process per video\n",
        "    'audio_sr': 16000,  # Audio sampling rate\n",
        "    'threshold': 0.5,  # Classification threshold (will be optimized)\n",
        "    'min_face_confidence': 0.9  # Minimum confidence for face detection\n",
        "}\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Configuration: {CONFIG}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvj2hGZTFEkQ"
      },
      "outputs": [],
      "source": [
        "def extract_frames(video_path, every_n=10, max_frames=30):\n",
        "\"\"\"Extract frames from video with error handling\"\"\"\n",
        "try:\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "print(f\"‚ö† Warning: Could not open video {video_path}\")\n",
        "return []\n",
        "    frames = []\n",
        "    frame_id = 0\n",
        "\n",
        "    while cap.isOpened() and len(frames) < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_id % every_n == 0:\n",
        "            frames.append(frame)\n",
        "        frame_id += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error extracting frames from {video_path}: {e}\")\n",
        "    return []\n",
        "def extract_audio(video_path, sr=16000):\n",
        "\"\"\"Extract audio from video with error handling\"\"\"\n",
        "try:\n",
        "y, sample_rate = librosa.load(video_path, sr=sr, duration=30) # Limit to 30 seconds\n",
        "return y, sample_rate\n",
        "except Exception as e:\n",
        "print(f\"‚ùå Error extracting audio from {video_path}: {e}\")\n",
        "return None, None\n",
        "\n",
        "def get_visual_emotions(frames):\n",
        "\"\"\"Extract visual emotions using FER (Facial Emotion Recognition)\"\"\"\n",
        "visual_emotions = []\n",
        "for frame in frames:\n",
        "    try:\n",
        "        # Detect emotions in frame\n",
        "        emotions = emotion_detector.detect_emotions(frame)\n",
        "\n",
        "        if emotions and len(emotions) > 0:\n",
        "            # Get the first detected face's emotions\n",
        "            emotion_dict = emotions[0]['emotions']\n",
        "            # Convert to numpy array in consistent order\n",
        "            emotion_vector = np.array([\n",
        "                emotion_dict.get('angry', 0),\n",
        "                emotion_dict.get('disgust', 0),\n",
        "                emotion_dict.get('fear', 0),\n",
        "                emotion_dict.get('happy', 0),\n",
        "                emotion_dict.get('sad', 0),\n",
        "                emotion_dict.get('surprise', 0),\n",
        "                emotion_dict.get('neutral', 0)\n",
        "            ])\n",
        "            visual_emotions.append(emotion_vector)\n",
        "\n",
        "    except Exception as e:\n",
        "        continue  # Skip problematic frames\n",
        "\n",
        "return visual_emotions\n",
        "def get_audio_emotion(audio_data, sr):\n",
        "\"\"\"Extract audio emotion features\"\"\"\n",
        "if audio_data is None or len(audio_data) == 0:\n",
        "return None\n",
        "try:\n",
        "    # Process audio in chunks if too long\n",
        "    max_duration = 10  # seconds\n",
        "    max_samples = max_duration * sr\n",
        "\n",
        "    if len(audio_data) > max_samples:\n",
        "        audio_data = audio_data[:max_samples]\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        audio_data,\n",
        "        sampling_rate=sr,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        max_length=max_samples,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = aem_model(**inputs).logits\n",
        "\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    return probs.squeeze().numpy()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error processing audio emotion: {e}\")\n",
        "    return None\n",
        "def calculate_cosine_similarities(audio_emotion, visual_emotions):\n",
        "\"\"\"Calculate cosine similarities between audio and visual emotions\"\"\"\n",
        "if audio_emotion is None or len(visual_emotions) == 0:\n",
        "return []\n",
        "similarities = []\n",
        "\n",
        "# Ensure audio emotion has same dimensionality as visual\n",
        "# If dimensions don't match, use feature engineering\n",
        "for vis_em in visual_emotions:\n",
        "    try:\n",
        "        # Normalize vectors\n",
        "        audio_norm = audio_emotion / (np.linalg.norm(audio_emotion) + 1e-8)\n",
        "        vis_norm = vis_em / (np.linalg.norm(vis_em) + 1e-8)\n",
        "\n",
        "        # If dimensions match, calculate cosine similarity\n",
        "        if len(audio_norm) == len(vis_norm):\n",
        "            sim = 1 - cosine(audio_norm, vis_norm)\n",
        "        else:\n",
        "            # Use correlation as alternative similarity measure\n",
        "            sim = np.corrcoef(\n",
        "                audio_norm[:min(len(audio_norm), len(vis_norm))],\n",
        "                vis_norm[:min(len(audio_norm), len(vis_norm))]\n",
        "            )[0, 1]\n",
        "            sim = (sim + 1) / 2  # Normalize to [0, 1]\n",
        "\n",
        "        similarities.append(sim)\n",
        "\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "return similarities\n",
        "print(\"‚úÖ Feature extraction functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A0FwQV8FRzB"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Cell 5: Main Processing Function\n",
        "\n",
        "```python\n",
        "def process_video(video_path, config):\n",
        "    \"\"\"Process a single video and extract features\"\"\"\n",
        "    result = {\n",
        "        'video': os.path.basename(video_path),\n",
        "        'avg_sim': 0,\n",
        "        'max_sim': 0,\n",
        "        'min_sim': 0,\n",
        "        'std_sim': 0,\n",
        "        'num_frames': 0,\n",
        "        'prediction': 'unknown',\n",
        "        'success': False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Extract frames\n",
        "        frames = extract_frames(\n",
        "            video_path,\n",
        "            every_n=config['frame_sampling_rate'],\n",
        "            max_frames=config['max_frames']\n",
        "        )\n",
        "\n",
        "        if len(frames) == 0:\n",
        "            print(f\"‚ö† No frames extracted from {os.path.basename(video_path)}\")\n",
        "            return result\n",
        "\n",
        "        result['num_frames'] = len(frames)\n",
        "\n",
        "        # Extract visual emotions\n",
        "        visual_emotions = get_visual_emotions(frames)\n",
        "\n",
        "        if len(visual_emotions) == 0:\n",
        "            print(f\"‚ö† No faces detected in {os.path.basename(video_path)}\")\n",
        "            return result\n",
        "\n",
        "        # Extract audio\n",
        "        audio_data, sr = extract_audio(video_path, sr=config['audio_sr'])\n",
        "\n",
        "        if audio_data is None:\n",
        "            print(f\"‚ö† No audio extracted from {os.path.basename(video_path)}\")\n",
        "            return result\n",
        "\n",
        "        # Get audio emotion\n",
        "        audio_emotion = get_audio_emotion(audio_data, sr)\n",
        "\n",
        "        if audio_emotion is None:\n",
        "            print(f\"‚ö† Could not process audio emotions for {os.path.basename(video_path)}\")\n",
        "            return result\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = calculate_cosine_similarities(audio_emotion, visual_emotions)\n",
        "\n",
        "        if len(similarities) == 0:\n",
        "            print(f\"‚ö† No similarities computed for {os.path.basename(video_path)}\")\n",
        "            return result\n",
        "\n",
        "        # Compute statistics\n",
        "        result['avg_sim'] = np.mean(similarities)\n",
        "        result['max_sim'] = np.max(similarities)\n",
        "        result['min_sim'] = np.min(similarities)\n",
        "        result['std_sim'] = np.std(similarities)\n",
        "        result['similarities'] = similarities\n",
        "        result['prediction'] = 'real' if result['avg_sim'] >= config['threshold'] else 'fake'\n",
        "        result['success'] = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {os.path.basename(video_path)}: {e}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"‚úÖ Processing function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa3EKnQQFo_g"
      },
      "outputs": [],
      "source": [
        "# Process all videos\n",
        "results = []\n",
        "failed_videos = []\n",
        "\n",
        "print(\"\\nüé¨ Processing videos...\\n\")\n",
        "\n",
        "for folder in video_folders:\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"‚ö† Folder not found: {folder_path}\")\n",
        "        continue\n",
        "\n",
        "    video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n",
        "    print(f\"üìÅ Processing folder {folder} ({label_map[folder]}): {len(video_files)} videos\")\n",
        "\n",
        "    for fname in tqdm(video_files, desc=f\"Folder {folder}\"):\n",
        "        video_path = os.path.join(folder_path, fname)\n",
        "        result = process_video(video_path, CONFIG)\n",
        "        result['ground_truth'] = label_map[folder]\n",
        "\n",
        "        if result['success']:\n",
        "            results.append(result)\n",
        "        else:\n",
        "            failed_videos.append(video_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully processed: {len(results)} videos\")\n",
        "print(f\"‚ùå Failed to process: {len(failed_videos)} videos\")\n",
        "\n",
        "if len(results) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è No videos were successfully processed. Please check your dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmS1AKQdGG2_"
      },
      "outputs": [],
      "source": [
        "# Optimize classification threshold using validation approach\n",
        "if len(results) > 10:  # Only if we have enough samples\n",
        "    print(\"\\nüéØ Optimizing classification threshold...\\n\")\n",
        "\n",
        "    # Split data for threshold optimization\n",
        "    train_results, val_results = train_test_split(results, test_size=0.3, random_state=42, stratify=[r['ground_truth'] for r in results])\n",
        "\n",
        "    best_threshold = CONFIG['threshold']\n",
        "    best_accuracy = 0\n",
        "    best_f1 = 0\n",
        "\n",
        "    thresholds = np.arange(0.3, 0.8, 0.05)\n",
        "    threshold_metrics = []\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        y_true = [r['ground_truth'] for r in val_results]\n",
        "        y_pred = ['real' if r['avg_sim'] >= thresh else 'fake' for r in val_results]\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        threshold_metrics.append({\n",
        "            'threshold': thresh,\n",
        "            'accuracy': acc,\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        })\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = thresh\n",
        "            best_accuracy = acc\n",
        "\n",
        "    # Update predictions with best threshold\n",
        "    CONFIG['threshold'] = best_threshold\n",
        "    for r in results:\n",
        "        r['prediction'] = 'real' if r['avg_sim'] >= best_threshold else 'fake'\n",
        "\n",
        "    print(f\"‚úÖ Optimal threshold: {best_threshold:.3f}\")\n",
        "    print(f\"   Validation Accuracy: {best_accuracy:.3f}\")\n",
        "    print(f\"   Validation F1-Score: {best_f1:.3f}\")\n",
        "\n",
        "    # Plot threshold analysis\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot([m['threshold'] for m in threshold_metrics], [m['accuracy'] for m in threshold_metrics], marker='o', label='Accuracy')\n",
        "    plt.plot([m['threshold'] for m in threshold_metrics], [m['f1'] for m in threshold_metrics], marker='s', label='F1-Score')\n",
        "    plt.axvline(x=best_threshold, color='red', linestyle='--', label=f'Best Threshold ({best_threshold:.3f})')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Threshold Optimization')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Not enough samples for threshold optimization. Using default threshold.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyPmRxccHSHw"
      },
      "outputs": [],
      "source": [
        "# Save results to CSV\n",
        "output_file = \"eava_results_enhanced.csv\"\n",
        "\n",
        "with open(output_file, \"w\", newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\n",
        "        \"video\", \"avg_similarity\", \"max_similarity\", \"min_similarity\",\n",
        "        \"std_similarity\", \"num_frames\", \"prediction\", \"ground_truth\"\n",
        "    ])\n",
        "\n",
        "    for r in results:\n",
        "        writer.writerow([\n",
        "            r['video'],\n",
        "            f\"{r['avg_sim']:.4f}\",\n",
        "            f\"{r['max_sim']:.4f}\",\n",
        "            f\"{r['min_sim']:.4f}\",\n",
        "            f\"{r['std_sim']:.4f}\",\n",
        "            r['num_frames'],\n",
        "            r['prediction'],\n",
        "            r['ground_truth']\n",
        "        ])\n",
        "\n",
        "print(f\"‚úÖ Results saved to {output_file}\")\n",
        "files.download(output_file)\n",
        "\n",
        "# Prepare data for evaluation\n",
        "y_true = [r['ground_truth'] for r in results]\n",
        "y_pred = [r['prediction'] for r in results]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
        "\n",
        "# Calculate additional metrics\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None, labels=['real', 'fake'], zero_division=0)\n",
        "\n",
        "print(f\"\\nüìà Detailed Metrics:\")\n",
        "print(f\"  Real    - Precision: {precision[0]:.4f}, Recall: {recall[0]:.4f}, F1: {f1[0]:.4f}\")\n",
        "print(f\"  Fake    - Precision: {precision[1]:.4f}, Recall: {recall[1]:.4f}, F1: {f1[1]:.4f}\")\n",
        "print(f\"  Overall Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoQu9_CzHdug"
      },
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"])\n",
        "\n",
        "# Plot confusion matrix with enhanced styling\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Raw counts\n",
        "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n",
        "disp1.plot(cmap='Blues', ax=ax1, values_format='d')\n",
        "ax1.set_title(f\"Confusion Matrix (Counts)\\nThreshold: {CONFIG['threshold']:.3f}\", fontsize=12, fontweight='bold')\n",
        "ax1.grid(False)\n",
        "\n",
        "# Normalized\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=[\"Real\", \"Fake\"])\n",
        "disp2.plot(cmap='Greens', ax=ax2, values_format='.2%')\n",
        "ax2.set_title(\"Confusion Matrix (Normalized)\", fontsize=12, fontweight='bold')\n",
        "ax2.grid(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print confusion matrix details\n",
        "print(\"\\nüìã Confusion Matrix Analysis:\")\n",
        "print(f\"  True Positives (Real correctly identified): {cm[0][0]}\")\n",
        "print(f\"  False Negatives (Real misclassified as Fake): {cm[0][1]}\")\n",
        "print(f\"  False Positives (Fake misclassified as Real): {cm[1][0]}\")\n",
        "print(f\"  True Negatives (Fake correctly identified): {cm[1][1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze cosine similarity distributions\n",
        "real_scores = [r['avg_sim'] for r in results if r['ground_truth'] == 'real']\n",
        "fake_scores = [r['avg_sim'] for r in results if r['ground_truth'] == 'fake']\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# 1. Histogram with KDE\n",
        "ax1 = axes[0, 0]\n",
        "sns.histplot(real_scores, label=\"Real\", color='green', kde=True, bins=15, alpha=0.6, ax=ax1)\n",
        "sns.histplot(fake_scores, label=\"Fake\", color='red', kde=True, bins=15, alpha=0.6, ax=ax1)\n",
        "ax1.axvline(x=CONFIG['threshold'], color='blue', linestyle='--', linewidth=2, label=f\"Threshold ({CONFIG['threshold']:.3f})\")\n",
        "ax1.set_title(\"Distribution of Average Cosine Similarities\", fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel(\"Average Cosine Similarity\")\n",
        "ax1.set_ylabel(\"Count\")\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Box plot\n",
        "ax2 = axes[0, 1]\n",
        "box_data = [real_scores, fake_scores]\n",
        "bp = ax2.boxplot(box_data, labels=['Real', 'Fake'], patch_artist=True, showmeans=True)\n",
        "bp['boxes'][0].set_facecolor('green')\n",
        "bp['boxes'][1].set_facecolor('red')\n",
        "for element in ['whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
        "    plt.setp(bp[element], color='black')\n",
        "ax2.axhline(y=CONFIG['threshold'], color='blue', linestyle='--', linewidth=2, label='Threshold')\n",
        "ax2.set_title(\"Cosine Similarity Box Plot\", fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel(\"Average Cosine Similarity\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Violin plot\n",
        "ax3 = axes[1, 0]\n",
        "violin_data = [real_scores, fake_scores]\n",
        "parts = ax3.violinplot(violin_data, positions=[1, 2], showmeans=True, showmedians=True)\n",
        "for pc in parts['bodies']:\n",
        "    pc.set_alpha(0.7)\n",
        "ax3.axhline(y=CONFIG['threshold'], color='blue', linestyle='--', linewidth=2, label='Threshold')\n",
        "ax3.set_xticks([1, 2])\n",
        "ax3.set_xticklabels(['Real', 'Fake'])\n",
        "ax3.set_title(\"Cosine Similarity Violin Plot\", fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel(\"Average Cosine Similarity\")\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 4. Scatter plot\n",
        "ax4 = axes[1, 1]\n",
        "real_indices = [i for i, r in enumerate(results) if r['ground_truth'] == 'real']\n",
        "fake_indices = [i for i, r in enumerate(results) if r['ground_truth'] == 'fake']\n",
        "ax4.scatter(real_indices, real_scores, color='green', alpha=0.6, label='Real', s=50)\n",
        "ax4.scatter(fake_indices, fake_scores, color='red', alpha=0.6, label='Fake', s=50)\n",
        "ax4.axhline(y=CONFIG['threshold'], color='blue', linestyle='--', linewidth=2, label='Threshold')\n",
        "ax4.set_title(\"Cosine Similarity Scatter Plot\", fontsize=12, fontweight='bold')\n",
        "ax4.set_xlabel(\"Video Index\")\n",
        "ax4.set_ylabel(\"Average Cosine Similarity\")\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical analysis\n",
        "print(\"\\nüìä Statistical Analysis of Cosine Similarities:\")\n",
        "print(f\"\\nReal Videos ({len(real_scores)} samples):\")\n",
        "print(f\"  Mean: {np.mean(real_scores):.4f}\")\n",
        "print(f\"  Median: {np.median(real_scores):.4f}\")\n",
        "print(f\"  Std Dev: {np.std(real_scores):.4f}\")\n",
        "print(f\"  Min: {np.min(real_scores):.4f}\")\n",
        "print(f\"  Max: {np.max(real_scores):.4f}\")\n",
        "\n",
        "print(f\"\\nFake Videos ({len(fake_scores)} samples):\")\n",
        "print(f\"  Mean: {np.mean(fake_scores):.4f}\")\n",
        "print(f\"  Median: {np.median(fake_scores):.4f}\")\n",
        "print(f\"  Std Dev: {np.std(fake_scores):.4f}\")\n",
        "print(f\"  Min: {np.min(fake_scores):.4f}\")\n",
        "print(f\"  Max: {np.max(fake_scores):.4f}\")\n",
        "\n",
        "# Statistical significance test\n",
        "from scipy import stats\n",
        "t_stat, p_value = stats.ttest_ind(real_scores, fake_scores)\n",
        "print(f\"\\nüî¨ T-test Results:\")\n",
        "print(f\"  t-statistic: {t_stat:.4f}\")\n",
        "print(f\"  p-value: {p_value:.6f}\")\n",
        "if p_value < 0.05:\n",
        "    print(\"  ‚úÖ Statistically significant difference (p < 0.05)\")\n",
        "else:\n",
        "    print(\"  ‚ö†Ô∏è No statistically significant difference (p >= 0.05)\")\n"
      ],
      "metadata": {
        "id": "fL1FcbZ51cWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class-wise accuracy\n",
        "real_acc = accuracy_score(\n",
        "    [r['ground_truth'] for r in results if r['ground_truth'] == 'real'],\n",
        "    [r['prediction'] for r in results if r['ground_truth'] == 'real']\n",
        ") if any(r['ground_truth'] == 'real' for r in results) else 0\n",
        "\n",
        "fake_acc = accuracy_score(\n",
        "    [r['ground_truth'] for r in results if r['ground_truth'] == 'fake'],\n",
        "    [r['prediction'] for r in results if r['ground_truth'] == 'fake']\n",
        ") if any(r['ground_truth'] == 'fake' for r in results) else 0\n",
        "\n",
        "# Visualize class-wise performance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy bar chart\n",
        "categories = ['Real', 'Fake', 'Overall']\n",
        "accuracies = [real_acc, fake_acc, accuracy_score(y_true, y_pred)]\n",
        "colors = ['green', 'red', 'blue']\n",
        "\n",
        "bars = ax1.bar(categories, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax1.set_ylim([0, 1])\n",
        "ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "ax1.set_title('Class-wise Accuracy', fontsize=12, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.3f}',\n",
        "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Metrics comparison\n",
        "metrics_data = {\n",
        "    'Real': [precision[0], recall[0], f1[0]],\n",
        "    'Fake': [precision[1], recall[1], f1[1]]\n",
        "}\n",
        "\n",
        "x = np.arange(3)\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax2.bar(x - width/2, metrics_data['Real'], width, label='Real', color='green', alpha=0.7, edgecolor='black')\n",
        "bars2 = ax2.bar(x + width/2, metrics_data['Fake'], width, label='Fake', color='red', alpha=0.7, edgecolor='black')\n",
        "\n",
        "ax2.set_ylabel('Score', fontsize=12)\n",
        "ax2.set_title('Precision, Recall, and F1-Score by Class', fontsize=12, fontweight='bold')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(['Precision', 'Recall', 'F1-Score'])\n",
        "ax2.legend()\n",
        "ax2.set_ylim([0, 1])\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ FINAL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Real Video Accuracy: {real_acc:.3f} ({sum(1 for r in results if r['ground_truth']=='real' and r['prediction']=='real')}/{sum(1 for r in results if r['ground_truth']=='real')})\")\n",
        "print(f\"‚ùå Fake Video Accuracy: {fake_acc:.3f} ({sum(1 for r in results if r['ground_truth']=='fake' and r['prediction']=='fake')}/{sum(1 for r in results if r['ground_truth']=='fake')})\")\n",
        "print(f\"üéØ Overall Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n",
        "print(f\"‚öñÔ∏è Balanced Accuracy: {(real_acc + fake_acc) / 2:.3f}\")\n",
        "print(f\"üéöÔ∏è Optimal Threshold: {CONFIG['threshold']:.3f}\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "eHwPxsaU1jBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cf34b15"
      },
      "source": [
        "# Task\n",
        "Use a pre-trained model for visual emotion recognition (e.g., FER+, EmoNet) to analyze emotions in video frames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "462e5779"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install any additional libraries required for the chosen pre-trained model (e.g., `torch`, `torchvision`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4429602"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the required libraries torch and torchvision using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c05f55f0"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d228430"
      },
      "source": [
        "## Download and load the pre-trained model\n",
        "\n",
        "### Subtask:\n",
        "Select a pre-trained model for visual emotion recognition (e.g., from Hugging Face or a similar repository), download it, and load it into the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7acec3cf"
      },
      "source": [
        "**Reasoning**:\n",
        "Select, download, and load a pre-trained model for visual emotion recognition. The `transformers` library is suitable for this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a1a4649"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "print(\"\\ud83d\\udd04 Loading visual emotion model...\")\n",
        "# Choose a suitable model. \"jonathangrosse/ferplus\" is a good option for facial emotion recognition.\n",
        "# Initialize the pipeline for facial emotion recognition\n",
        "ver_model = pipeline(\"image-classification\", model=\"jonathangrosse/ferplus\")\n",
        "\n",
        "print(\"‚úÖ Visual emotion model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d235732"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the visual emotion model failed because the specified model ID was not found on Hugging Face. I need to try loading the model with a different and valid model ID. I will try using a known model for facial expression recognition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PHY8Xiknu50"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "print(\"\\ud83d\\udd04 Loading visual emotion model...\")\n",
        "# Choose a suitable model. \"trpakov/fer_tiny_vit\" is another option for facial emotion recognition.\n",
        "model_name = \"trpakov/fer_tiny_vit\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "ver_model = AutoModelForImageClassification.from_pretrained(model_name)\n",
        "\n",
        "print(\"‚úÖ Visual emotion model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e0c456b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the visual emotion model also failed, likely due to the same issue with the model ID or access. I will try a different, well-known model for facial expression analysis that is likely publicly available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmxCpUMJnxWw"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "print(\"\\ud83d\\udd04 Loading visual emotion model...\")\n",
        "# Try a more commonly used model for facial expression analysis.\n",
        "# \"dima806/facial_emotion_recognition_cnn\" is another potential option.\n",
        "ver_model = pipeline(\"image-classification\", model=\"dima806/facial_emotion_recognition_cnn\")\n",
        "\n",
        "print(\"‚úÖ Visual emotion model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08db577d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load visual emotion models from Hugging Face have failed, likely due to accessibility issues with the specified model IDs. I will try a more general image classification model that can be fine-tuned or adapted for emotion recognition, which might be more readily available. I will also ensure necessary libraries are imported.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbFOIk1cnzWM"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoModelForImageClassification, AutoImageProcessor\n",
        "\n",
        "print(\"\\ud83d\\udd04 Loading a general image classification model...\")\n",
        "# Using a general image classification model like \"google/vit-base-patch16-224\" and its processor.\n",
        "# This can be adapted for emotion recognition if the model is pre-trained on a diverse dataset.\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "ver_model = AutoModelForImageClassification.from_pretrained(model_name)\n",
        "\n",
        "print(\"‚úÖ Image classification model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63636285"
      },
      "source": [
        "## Modify the predict visual emotion function\n",
        "\n",
        "### Subtask:\n",
        "Update the `predict_visual_emotion` function to use the loaded pre-trained model to predict emotions from face images. This will likely involve preprocessing the face images according to the model's requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32cb9e00"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `predict_visual_emotion` function to use the loaded `processor` and `ver_model` for emotion prediction, following the instructions to preprocess the image and return the model's raw output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79ca6dc9"
      },
      "outputs": [],
      "source": [
        "def predict_visual_emotion(face_img):\n",
        "    \"\"\"\n",
        "    Predicts visual emotion from a face image using the loaded pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        face_img: A numpy array representing the face image (BGR format).\n",
        "\n",
        "    Returns:\n",
        "        A numpy array of logits or probabilities from the model's output.\n",
        "    \"\"\"\n",
        "    # Convert BGR numpy array to RGB PIL Image\n",
        "    face_img_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
        "    pil_img = Image.fromarray(face_img_rgb)\n",
        "\n",
        "    # Preprocess the image using the loaded processor\n",
        "    inputs = processor(images=pil_img, return_tensors=\"pt\")\n",
        "\n",
        "    # Pass the processed image through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = ver_model(**inputs)\n",
        "\n",
        "    # Return the raw logits or probabilities\n",
        "    # The output format depends on the specific model, often it's logits\n",
        "    return outputs.logits.squeeze().numpy()\n",
        "\n",
        "print(\"‚úÖ predict_visual_emotion function updated to use the pre-trained model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28f91089"
      },
      "source": [
        "## Update the get visual emotions function\n",
        "\n",
        "### Subtask:\n",
        "Modify the `get_visual_emotions` function to handle the output of the new `predict_visual_emotion` function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47c91348"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `get_visual_emotions` function to use the updated `predict_visual_emotion` function and collect its output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91f532a9"
      },
      "outputs": [],
      "source": [
        "def get_visual_emotions(frames):\n",
        "    \"\"\"\n",
        "    Extracts visual emotions from a list of video frames.\n",
        "\n",
        "    Args:\n",
        "        frames: A list of numpy arrays, where each array is a video frame (BGR format).\n",
        "\n",
        "    Returns:\n",
        "        A list of numpy arrays, where each array contains the visual emotion prediction\n",
        "        (logits or probabilities) for a detected face in a frame.\n",
        "    \"\"\"\n",
        "    visual_emotions = []\n",
        "    for frame in frames:\n",
        "        # Detect faces in the frame\n",
        "        boxes, _ = mtcnn.detect(frame)\n",
        "\n",
        "        if boxes is not None:\n",
        "            # Assuming we only process the first detected face\n",
        "            x1, y1, x2, y2 = boxes[0].astype(int)\n",
        "            face = frame[y1:y2, x1:x2]\n",
        "\n",
        "            if face.size == 0:\n",
        "                # Skip if the detected face region is empty\n",
        "                continue\n",
        "\n",
        "            # Use the updated predict_visual_emotion function to get model output\n",
        "            em = predict_visual_emotion(face)\n",
        "\n",
        "            # Append the model's output (logits/probabilities)\n",
        "            visual_emotions.append(em)\n",
        "\n",
        "    return visual_emotions\n",
        "\n",
        "print(\"‚úÖ get_visual_emotions function updated to handle new predict_visual_emotion output.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6c3acf4"
      },
      "source": [
        "## Rerun the analysis\n",
        "\n",
        "### Subtask:\n",
        "Execute the cells that perform the analysis and generate the results using the updated visual emotion recognition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aae42ae"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell containing the `run_eava` function and the loop to process videos failed. The error messages indicate issues with character encoding during file operations and potentially with the `librosa.load` function falling back to `audioread`. The `run_eava` function itself also needs to be re-defined as it was overwritten in the previous failed cell execution. The goal is to re-execute the analysis process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dc63494"
      },
      "outputs": [],
      "source": [
        "# Redefine run_eava with the plot generation included\n",
        "def run_eava(video_path):\n",
        "    print(f\"\\nProcessing: {video_path}\")\n",
        "\n",
        "    frames = extract_frames(video_path)\n",
        "    visual_emotions = get_visual_emotions(frames)\n",
        "\n",
        "    try:\n",
        "        y, sr = extract_audio(video_path)\n",
        "        audio_emotion = get_audio_emotion(y, sr)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting or processing audio for {video_path}: {e}\")\n",
        "        # Handle cases where audio extraction or processing fails\n",
        "        return {\n",
        "            \"video\": os.path.basename(video_path),\n",
        "            \"avg_sim\": 0, # Default to 0 similarity if audio fails\n",
        "            \"prediction\": \"fake\", # Default to fake if cannot process audio\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "    similarities = []\n",
        "    # Assuming visual_emotions is a list of numpy arrays (logits)\n",
        "    # We need to apply softmax or similar to get probabilities if the model doesn't output them directly\n",
        "    # And align the dimensions for cosine similarity. This depends on the specific model output.\n",
        "    # Since we don't know the exact output shape/meaning of the new visual model,\n",
        "    # we will need to make an assumption or adjust based on the model's documentation.\n",
        "    # Let's assume for now the model outputs a vector of scores that can be compared with the audio emotion vector.\n",
        "    # If the visual model outputs logits for many classes, we might need to map them to a similar\n",
        "    # emotion space as the audio model or find a common representation.\n",
        "\n",
        "    # **Crucially, the visual emotion model output needs to be compatible with the audio emotion output**\n",
        "    # The previous predict_visual_emotion returned raw logits.\n",
        "    # The audio emotion model returns probabilities (after softmax).\n",
        "    # We need to ensure we are comparing comparable representations.\n",
        "    # Let's assume for the sake of proceeding that the raw logits from the visual model\n",
        "    # can be directly compared with the audio probabilities after normalization or a similar step.\n",
        "    # This is a strong assumption and likely needs refinement based on the models used.\n",
        "\n",
        "    # For demonstration, let's just compare the raw visual model outputs with the audio probabilities.\n",
        "    # This might not be a valid comparison depending on the models.\n",
        "    # A better approach would be to align the emotion spaces or use a model specifically trained for cross-modal similarity.\n",
        "\n",
        "    # Let's check the shape of visual_emotions[0] and audio_emotion to see if they can be compared.\n",
        "    # If they have different dimensions or represent different things, cosine similarity is not appropriate.\n",
        "\n",
        "    # Assuming `visual_emotions[0]` and `audio_emotion` are comparable vectors:\n",
        "    if visual_emotions and audio_emotion is not None:\n",
        "        # Simple check: are the dimensions the same?\n",
        "        if visual_emotions[0].shape == audio_emotion.shape:\n",
        "            for vis_em in visual_emotions:\n",
        "                # Ensure inputs to cosine are 1-D arrays\n",
        "                if vis_em.ndim > 1:\n",
        "                   vis_em = vis_em.flatten()\n",
        "                if audio_emotion.ndim > 1:\n",
        "                    audio_emotion = audio_emotion.flatten()\n",
        "\n",
        "                # Ensure inputs are not zero vectors, which causes issues with cosine similarity\n",
        "                if np.linalg.norm(vis_em) > 1e-9 and np.linalg.norm(audio_emotion) > 1e-9:\n",
        "                    try:\n",
        "                        sim = 1 - cosine(audio_emotion, vis_em)\n",
        "                        similarities.append(sim)\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Cosine similarity error for {video_path}: {e}\")\n",
        "                        similarities.append(0) # Append 0 similarity on error\n",
        "                else:\n",
        "                     similarities.append(0) # Append 0 similarity if either vector is zero\n",
        "\n",
        "        else:\n",
        "            print(f\"Dimension mismatch for {video_path}: Visual emotion shape {visual_emotions[0].shape}, Audio emotion shape {audio_emotion.shape}\")\n",
        "            # Cannot compute meaningful similarity with dimension mismatch\n",
        "            similarities = [0] # Default to 0 similarity\n",
        "\n",
        "    avg_sim = np.mean(similarities) if similarities else 0\n",
        "    label = 'real' if avg_sim >= 0.6 else 'fake' # Keep the threshold for now\n",
        "\n",
        "    if similarities:\n",
        "        try:\n",
        "            plt.figure(figsize=(8, 3))\n",
        "            plt.plot(similarities, marker='o')\n",
        "            plt.axhline(y=0.6, color='red', linestyle='--')\n",
        "            title_safe = f\"Emotion Similarity per Frame\\n{os.path.basename(video_path)} -> {safe_string(label.upper())}\"\n",
        "            plt.title(title_safe)\n",
        "            plt.xlabel(\"Frame Index\")\n",
        "            plt.ylabel(\"Cosine Similarity\")\n",
        "            plt.ylim(0, 1)\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "        except UnicodeEncodeError:\n",
        "            print(f\"Could not generate plot for {os.path.basename(video_path)} due to character encoding issues.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating plot for {os.path.basename(video_path)}: {e}\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"video\": os.path.basename(video_path),\n",
        "        \"avg_sim\": avg_sim,\n",
        "        \"prediction\": label\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "for folder in video_folders:\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        for fname in os.listdir(folder_path):\n",
        "            # Add a check to limit the number of videos processed if needed for faster execution\n",
        "            # if len(results) >= 20: # Process only 20 videos for quick test\n",
        "            #     break\n",
        "            if fname.endswith(\".mp4\"):\n",
        "                video_path = os.path.join(folder_path, fname)\n",
        "                res = run_eava(video_path)\n",
        "                res['ground_truth'] = label_map[folder]\n",
        "                results.append(res)\n",
        "        # if len(results) >= 20 and 'break' in locals(): # Break outer loop too if limit is set\n",
        "        #     break\n",
        "    else:\n",
        "        print(f\"Folder not found: {folder_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21526ab5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because the variables `base_path`, `video_folders`, and `label_map` were not defined in the current session, likely due to the failure in the cell where they were initially defined (cell ID `8s0nimIfD19x`). To proceed with the analysis, these variables need to be redefined.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDm5mqT-oGWg"
      },
      "outputs": [],
      "source": [
        "# Redefine necessary variables from the previously failed cell\n",
        "base_path = \"FakeAVCeleb\"\n",
        "video_folders = [\"A\", \"B\", \"C\", \"D\"]\n",
        "label_map = {\"A\": \"real\", \"B\": \"fake\", \"C\": \"fake\", \"D\": \"fake\"}\n",
        "\n",
        "# Redefine run_eava with the plot generation included\n",
        "def run_eava(video_path):\n",
        "    print(f\"\\nProcessing: {video_path}\")\n",
        "\n",
        "    frames = extract_frames(video_path)\n",
        "    visual_emotions = get_visual_emotions(frames)\n",
        "\n",
        "    try:\n",
        "        y, sr = extract_audio(video_path)\n",
        "        audio_emotion = get_audio_emotion(y, sr)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting or processing audio for {video_path}: {e}\")\n",
        "        # Handle cases where audio extraction or processing fails\n",
        "        return {\n",
        "            \"video\": os.path.basename(video_path),\n",
        "            \"avg_sim\": 0, # Default to 0 similarity if audio fails\n",
        "            \"prediction\": \"fake\", # Default to fake if cannot process audio\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "    similarities = []\n",
        "    # Assuming visual_emotions is a list of numpy arrays (logits)\n",
        "    # We need to apply softmax or similar to get probabilities if the model doesn't output them directly\n",
        "    # And align the dimensions for cosine similarity. This depends on the specific model output.\n",
        "    # Since we don't know the exact output shape/meaning of the new visual model,\n",
        "    # we will need to make an assumption or adjust based on the model's documentation.\n",
        "    # Let's assume for now the model outputs a vector of scores that can be compared with the audio emotion vector.\n",
        "    # If the visual model outputs logits for many classes, we might need to map them to a similar\n",
        "    # emotion space as the audio model or find a common representation.\n",
        "\n",
        "    # **Crucially, the visual emotion model output needs to be compatible with the audio emotion output**\n",
        "    # The previous predict_visual_emotion returned raw logits.\n",
        "    # The audio emotion model returns probabilities (after softmax).\n",
        "    # We need to ensure we are comparing comparable representations.\n",
        "    # Let's assume for the sake of proceeding that the raw logits from the visual model\n",
        "    # can be directly compared with the audio probabilities after normalization or a similar step.\n",
        "    # This is a strong assumption and likely needs refinement based on the models used.\n",
        "\n",
        "    # For demonstration, let's just compare the raw visual model outputs with the audio probabilities.\n",
        "    # This might not be a valid comparison depending on the models.\n",
        "    # A better approach would be to align the emotion spaces or use a model specifically trained for cross-modal similarity.\n",
        "\n",
        "    # Let's check the shape of visual_emotions[0] and audio_emotion to see if they can be compared.\n",
        "    # If they have different dimensions or represent different things, cosine similarity is not appropriate.\n",
        "\n",
        "    # Assuming `visual_emotions[0]` and `audio_emotion` are comparable vectors:\n",
        "    if visual_emotions and audio_emotion is not None:\n",
        "        # Simple check: are the dimensions the same?\n",
        "        if visual_emotions[0].shape == audio_emotion.shape:\n",
        "            for vis_em in visual_emotions:\n",
        "                # Ensure inputs to cosine are 1-D arrays\n",
        "                if vis_em.ndim > 1:\n",
        "                   vis_em = vis_em.flatten()\n",
        "                if audio_emotion.ndim > 1:\n",
        "                    audio_emotion = audio_emotion.flatten()\n",
        "\n",
        "                # Ensure inputs are not zero vectors, which causes issues with cosine similarity\n",
        "                if np.linalg.norm(vis_em) > 1e-9 and np.linalg(audio_emotion) > 1e-9:\n",
        "                    try:\n",
        "                        sim = 1 - cosine(audio_emotion, vis_em)\n",
        "                        similarities.append(sim)\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Cosine similarity error for {video_path}: {e}\")\n",
        "                        similarities.append(0) # Append 0 similarity on error\n",
        "                else:\n",
        "                     similarities.append(0) # Append 0 similarity if either vector is zero\n",
        "\n",
        "        else:\n",
        "            print(f\"Dimension mismatch for {video_path}: Visual emotion shape {visual_emotions[0].shape}, Audio emotion shape {audio_emotion.shape}\")\n",
        "            # Cannot compute meaningful similarity with dimension mismatch\n",
        "            similarities = [0] # Default to 0 similarity\n",
        "\n",
        "\n",
        "    avg_sim = np.mean(similarities) if similarities else 0\n",
        "    label = 'real' if avg_sim >= 0.6 else 'fake' # Keep the threshold for now\n",
        "\n",
        "    if similarities:\n",
        "        try:\n",
        "            plt.figure(figsize=(8, 3))\n",
        "            plt.plot(similarities, marker='o')\n",
        "            plt.axhline(y=0.6, color='red', linestyle='--')\n",
        "            title_safe = f\"Emotion Similarity per Frame\\n{os.path.basename(video_path)} -> {safe_string(label.upper())}\"\n",
        "            plt.title(title_safe)\n",
        "            plt.xlabel(\"Frame Index\")\n",
        "            plt.ylabel(\"Cosine Similarity\")\n",
        "            plt.ylim(0, 1)\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "        except UnicodeEncodeError:\n",
        "            print(f\"Could not generate plot for {os.path.basename(video_path)} due to character encoding issues.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating plot for {os.path.basename(video_path)}: {e}\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"video\": os.path.basename(video_path),\n",
        "        \"avg_sim\": avg_sim,\n",
        "        \"prediction\": label\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "for folder in video_folders:\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        for fname in os.listdir(folder_path):\n",
        "            # Add a check to limit the number of videos processed if needed for faster execution\n",
        "            # if len(results) >= 20: # Process only 20 videos for quick test\n",
        "            #     break\n",
        "            if fname.endswith(\".mp4\"):\n",
        "                video_path = os.path.join(folder_path, fname)\n",
        "                res = run_eava(video_path)\n",
        "                res['ground_truth'] = label_map[folder]\n",
        "                results.append(res)\n",
        "        # if len(results) >= 20 and 'break' in locals(): # Break outer loop too if limit is set\n",
        "        #     break\n",
        "    else:\n",
        "        print(f\"Folder not found: {folder_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd73f2c4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because the `os` module was not imported in the current session. It was imported in a previous cell that was not re-executed. To fix this and continue with the analysis, the `os` module needs to be imported.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZYKV5-ioJrA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Redefine necessary variables from the previously failed cell\n",
        "base_path = \"FakeAVCeleb\"\n",
        "video_folders = [\"A\", \"B\", \"C\", \"D\"]\n",
        "label_map = {\"A\": \"real\", \"B\": \"fake\", \"C\": \"fake\", \"D\": \"fake\"}\n",
        "\n",
        "# Redefine run_eava with the plot generation included\n",
        "def run_eava(video_path):\n",
        "    print(f\"\\nProcessing: {video_path}\")\n",
        "\n",
        "    frames = extract_frames(video_path)\n",
        "    visual_emotions = get_visual_emotions(frames)\n",
        "\n",
        "    try:\n",
        "        y, sr = extract_audio(video_path)\n",
        "        audio_emotion = get_audio_emotion(y, sr)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting or processing audio for {video_path}: {e}\")\n",
        "        # Handle cases where audio extraction or processing fails\n",
        "        return {\n",
        "            \"video\": os.path.basename(video_path),\n",
        "            \"avg_sim\": 0, # Default to 0 similarity if audio fails\n",
        "            \"prediction\": \"fake\", # Default to fake if cannot process audio\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "    similarities = []\n",
        "    # Assuming visual_emotions is a list of numpy arrays (logits)\n",
        "    # We need to apply softmax or similar to get probabilities if the model doesn't output them directly\n",
        "    # And align the dimensions for cosine similarity. This depends on the specific model output.\n",
        "    # Since we don't know the exact output shape/meaning of the new visual model,\n",
        "    # we will need to make an assumption or adjust based on the model's documentation.\n",
        "    # Let's assume for now the model outputs a vector of scores that can be compared with the audio emotion vector.\n",
        "    # If the visual model outputs logits for many classes, we might need to map them to a similar\n",
        "    # emotion space as the audio model or find a common representation.\n",
        "\n",
        "    # **Crucially, the visual emotion model output needs to be compatible with the audio emotion output**\n",
        "    # The previous predict_visual_emotion returned raw logits.\n",
        "    # The audio emotion model returns probabilities (after softmax).\n",
        "    # We need to ensure we are comparing comparable representations.\n",
        "    # Let's assume for the sake of proceeding that the raw logits from the visual model\n",
        "    # can be directly compared with the audio probabilities after normalization or a similar step.\n",
        "    # This is a strong assumption and likely needs refinement based on the models used.\n",
        "\n",
        "    # For demonstration, let's just compare the raw visual model outputs with the audio probabilities.\n",
        "    # This might not be a valid comparison depending on the models.\n",
        "    # A better approach would be to align the emotion spaces or use a model specifically trained for cross-modal similarity.\n",
        "\n",
        "    # Let's check the shape of visual_emotions[0] and audio_emotion to see if they can be compared.\n",
        "    # If they have different dimensions or represent different things, cosine similarity is not appropriate.\n",
        "\n",
        "    # Assuming `visual_emotions[0]` and `audio_emotion` are comparable vectors:\n",
        "    if visual_emotions and audio_emotion is not None:\n",
        "        # Simple check: are the dimensions the same?\n",
        "        if visual_emotions[0].shape == audio_emotion.shape:\n",
        "            for vis_em in visual_emotions:\n",
        "                # Ensure inputs to cosine are 1-D arrays\n",
        "                if vis_em.ndim > 1:\n",
        "                   vis_em = vis_em.flatten()\n",
        "                if audio_emotion.ndim > 1:\n",
        "                    audio_emotion = audio_emotion.flatten()\n",
        "\n",
        "                # Ensure inputs are not zero vectors, which causes issues with cosine similarity\n",
        "                if np.linalg.norm(vis_em) > 1e-9 and np.linalg.norm(audio_emotion) > 1e-9:\n",
        "                    try:\n",
        "                        sim = 1 - cosine(audio_emotion, vis_em)\n",
        "                        similarities.append(sim)\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Cosine similarity error for {video_path}: {e}\")\n",
        "                        similarities.append(0) # Append 0 similarity on error\n",
        "                else:\n",
        "                     similarities.append(0) # Append 0 similarity if either vector is zero\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(f\"Dimension mismatch for {video_path}: Visual emotion shape {visual_emotions[0].shape}, Audio emotion shape {audio_emotion.shape}\")\n",
        "            # Cannot compute meaningful similarity with dimension mismatch\n",
        "            similarities = [0] # Default to 0 similarity\n",
        "\n",
        "\n",
        "    avg_sim = np.mean(similarities) if similarities else 0\n",
        "    label = 'real' if avg_sim >= 0.6 else 'fake' # Keep the threshold for now\n",
        "\n",
        "    if similarities:\n",
        "        try:\n",
        "            plt.figure(figsize=(8, 3))\n",
        "            plt.plot(similarities, marker='o')\n",
        "            plt.axhline(y=0.6, color='red', linestyle='--')\n",
        "            title_safe = f\"Emotion Similarity per Frame\\n{os.path.basename(video_path)} -> {safe_string(label.upper())}\"\n",
        "            plt.title(title_safe)\n",
        "            plt.xlabel(\"Frame Index\")\n",
        "            plt.ylabel(\"Cosine Similarity\")\n",
        "            plt.ylim(0, 1)\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "        except UnicodeEncodeError:\n",
        "            print(f\"Could not generate plot for {os.path.basename(video_path)} due to character encoding issues.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating plot for {os.path.basename(video_path)}: {e}\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"video\": os.path.basename(video_path),\n",
        "        \"avg_sim\": avg_sim,\n",
        "        \"prediction\": label\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "for folder in video_folders:\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        for fname in os.listdir(folder_path):\n",
        "            # Add a check to limit the number of videos processed if needed for faster execution\n",
        "            # if len(results) >= 20: # Process only 20 videos for quick test\n",
        "            #     break\n",
        "            if fname.endswith(\".mp4\"):\n",
        "                video_path = os.path.join(folder_path, fname)\n",
        "                res = run_eava(video_path)\n",
        "                res['ground_truth'] = label_map[folder]\n",
        "                results.append(res)\n",
        "        # if len(results) >= 20 and 'break' in locals(): # Break outer loop too if limit is set\n",
        "        #     break\n",
        "    else:\n",
        "        print(f\"Folder not found: {folder_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd36efe5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution showed that the video folders (`FakeAVCeleb/A`, `FakeAVCeleb/B`, etc.) were not found. This is likely because the file upload and extraction step (cell ID `8s0nimIfD19x`) failed previously and was not successfully re-executed. To fix this, I need to re-run the code that handles the file upload and extraction of the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c810cbb"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile, os\n",
        "\n",
        "print(\"\\ud83d\\udce4 Please upload FakeAVCeleb.zip...\")\n",
        "try:\n",
        "    # Attempt to upload the file\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Check if any file was uploaded\n",
        "    if uploaded:\n",
        "        zip_file = list(uploaded.keys())[0]\n",
        "        print(f\"Uploaded file: {zip_file}\")\n",
        "\n",
        "        # Extract the zip file\n",
        "        print(f\"\\ud83d\\udd0e Extracting {zip_file}...\")\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\".\")\n",
        "        print(\"‚úÖ Extraction complete.\")\n",
        "\n",
        "        # Verify extraction by checking if base_path exists\n",
        "        base_path = \"FakeAVCeleb\"\n",
        "        if os.path.exists(base_path):\n",
        "            print(f\"‚úÖ Extracted folder '{base_path}' found.\")\n",
        "        else:\n",
        "            print(f\"‚ùå Extracted folder '{base_path}' not found.\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå No file was uploaded.\")\n",
        "\n",
        "except UnicodeEncodeError:\n",
        "    print(\"An error occurred during file upload due to character encoding.\")\n",
        "    print(\"Please ensure the filename and content do not contain special characters that cannot be encoded in UTF-8.\")\n",
        "    print(\"You may need to upload the file manually to your Colab environment.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during file upload or extraction: {e}\")\n",
        "\n",
        "\n",
        "# Define base path and video folders - these were lost in previous failed runs\n",
        "base_path = \"FakeAVCeleb\"\n",
        "video_folders = [\"A\", \"B\", \"C\", \"D\"]\n",
        "label_map = {\"A\": \"real\", \"B\": \"fake\", \"C\": \"fake\", \"D\": \"fake\"}\n",
        "\n",
        "print(\"\\ud83d\\udcd1 Variables 'base_path', 'video_folders', and 'label_map' defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd593792"
      },
      "source": [
        "\n",
        "# üöÄ Performance Improvements Added\n",
        "\n",
        "This section integrates several manipulations to improve FakeAVCeleb deepfake detection accuracy:\n",
        "1. **Multiple frame sampling** per video (for stable face embeddings).\n",
        "2. **Audio augmentation** (noise, pitch, speed).\n",
        "3. **Fine-tuning support for Wav2Vec2** (optional GPU-based training).\n",
        "4. **Fusion network** combining audio and visual embeddings.\n",
        "5. **Normalization and evaluation updates**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2f70672"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch, cv2, librosa, numpy as np, random\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# Initialize models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mtcnn = MTCNN(keep_all=False, post_process=True, device=device)\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
        "\n",
        "def sample_frames(video_path, n=10):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    idxs = np.linspace(0, frame_count - 1, n).astype(int)\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def get_face_embedding(frame):\n",
        "    with torch.no_grad():\n",
        "        face = mtcnn(frame)\n",
        "        if face is not None:\n",
        "            emb = resnet(face.unsqueeze(0).to(device))\n",
        "            return emb.squeeze().cpu().numpy()\n",
        "    return None\n",
        "\n",
        "def augment_audio(y, sr):\n",
        "    if random.random() < 0.3:\n",
        "        y = librosa.effects.pitch_shift(y, sr, n_steps=random.uniform(-2, 2))\n",
        "    if random.random() < 0.3:\n",
        "        y = librosa.effects.time_stretch(y, rate=random.uniform(0.9, 1.1))\n",
        "    if random.random() < 0.3:\n",
        "        y = y + 0.005 * np.random.randn(len(y))\n",
        "    return y\n",
        "\n",
        "def extract_audio_embedding(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "    y = augment_audio(y, sr)\n",
        "    inputs = feature_extractor(y, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = audio_model(**{k: v.to(device) for k, v in inputs.items()})\n",
        "    emb = outputs.logits.squeeze().cpu().numpy()\n",
        "    return emb / np.linalg.norm(emb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac46a069"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class FusionNet(nn.Module):\n",
        "    def __init__(self, a_dim=768, v_dim=512):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(a_dim + v_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "    def forward(self, a, v):\n",
        "        x = torch.cat((a, v), dim=1)\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5899d264"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example fusion of features from one sample video and audio file\n",
        "\n",
        "video_path = \"FakeAVCeleb/A/sample.mp4\"  # replace dynamically in your loop\n",
        "audio_path = \"FakeAVCeleb/A/sample.wav\"\n",
        "\n",
        "frames = sample_frames(video_path, n=10)\n",
        "face_embs = [get_face_embedding(f) for f in frames if get_face_embedding(f) is not None]\n",
        "if face_embs:\n",
        "    visual_emb = np.mean(face_embs, axis=0)\n",
        "else:\n",
        "    visual_emb = np.zeros(512)\n",
        "\n",
        "audio_emb = extract_audio_embedding(audio_path)\n",
        "\n",
        "# Normalize embeddings\n",
        "visual_emb = visual_emb / np.linalg.norm(visual_emb)\n",
        "audio_emb = audio_emb / np.linalg.norm(audio_emb)\n",
        "\n",
        "fusion_model = FusionNet().to(device)\n",
        "fusion_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    a = torch.tensor(audio_emb).float().unsqueeze(0).to(device)\n",
        "    v = torch.tensor(visual_emb).float().unsqueeze(0).to(device)\n",
        "    out = fusion_model(a, v)\n",
        "    pred = torch.argmax(out, dim=1).item()\n",
        "\n",
        "print(\"Prediction:\", \"Real\" if pred == 0 else \"Fake\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33d46148"
      },
      "source": [
        "\n",
        "# üîÅ Final Integrated Pipeline: Fine-tuning + Evaluation\n",
        "\n",
        "This notebook adds:\n",
        "\n",
        "- Subset control for processing FakeAVCeleb (to avoid running out of time/storage)\n",
        "- Audio dataset class and **Wav2Vec2** fine-tuning using Hugging Face `Trainer`\n",
        "- Embedding extraction loop for video (multiple-frame sampling + face embeddings)\n",
        "- Cosine similarity computations (as in original notebook)\n",
        "- Confusion matrix, classification report and t-SNE visualization of embeddings\n",
        "\n",
        "> **Note:** Training/fine-tuning can be time-consuming. Use `max_videos_per_class` to limit dataset size for quick experimentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "358a6f1b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === PARAMETERS ===\n",
        "DATA_ROOT = \"FakeAVCeleb\"   # path to unzipped FakeAVCeleb folder in the environment\n",
        "MAX_VIDEOS_PER_CLASS = 100  # set to None to use all files (careful with disk/runtime limits)\n",
        "SAMPLE_FRAMES = 12          # number of frames sampled per video\n",
        "AUDIO_SR = 16000\n",
        "DEVICE = \"cuda\" if __import__('torch').cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"DEVICE:\", DEVICE)\n",
        "print(\"DATA_ROOT:\", DATA_ROOT)\n",
        "print(\"MAX_VIDEOS_PER_CLASS:\", MAX_VIDEOS_PER_CLASS)\n",
        "print(\"SAMPLE_FRAMES:\", SAMPLE_FRAMES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6c5c485"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Dataset listing (with subset control) ===\n",
        "import os, glob, random, math\n",
        "from pathlib import Path\n",
        "\n",
        "classes = {\"A\":0, \"B\":1, \"C\":1, \"D\":1}  # A -> real (0), others -> fake (1)\n",
        "\n",
        "video_list = []\n",
        "for cls_folder, label in classes.items():\n",
        "    folder = os.path.join(DATA_ROOT, cls_folder)\n",
        "    if not os.path.exists(folder):\n",
        "        print(f\"Warning: folder {folder} not found.\")\n",
        "        continue\n",
        "    vids = sorted(glob.glob(os.path.join(folder, \"*.*\")))\n",
        "    # filter common video extensions\n",
        "    vids = [v for v in vids if v.split('.')[-1].lower() in ['mp4','avi','mov','mkv','webm']]\n",
        "    if MAX_VIDEOS_PER_CLASS is not None:\n",
        "        vids = vids[:MAX_VIDEOS_PER_CLASS]\n",
        "    for v in vids:\n",
        "        video_list.append({\"path\": v, \"label\": label, \"class_folder\": cls_folder})\n",
        "\n",
        "print(f\"Found {len(video_list)} videos (subset applied). Example entries:\")\n",
        "video_list[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06985e83"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Helper functions: frame sampling, face embedding, audio extraction ===\n",
        "import cv2, numpy as np, torch, librosa\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# initialize models (reuse from previous cells if present)\n",
        "device = torch.device(DEVICE)\n",
        "mtcnn = MTCNN(keep_all=False, post_process=True, device=device)\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "# base model used for embedding extraction; will be replaced/overwritten by fine-tuned model later\n",
        "audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
        "\n",
        "def sample_frames(video_path, n=SAMPLE_FRAMES):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        return []\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
        "    if frame_count == 0:\n",
        "        cap.release()\n",
        "        return []\n",
        "    idxs = np.linspace(0, frame_count - 1, n).astype(int)\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def get_face_embedding(frame):\n",
        "    with torch.no_grad():\n",
        "        face = mtcnn(frame)\n",
        "        if face is not None:\n",
        "            emb = resnet(face.unsqueeze(0).to(device))\n",
        "            emb = emb.squeeze().cpu().numpy()\n",
        "            return emb\n",
        "    return None\n",
        "\n",
        "def extract_audio_embedding_from_file(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=AUDIO_SR)\n",
        "    # no augmentation here - used for deterministic embedding extraction\n",
        "    inputs = feature_extractor(y, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = audio_model(**{k:v.to(device) for k,v in inputs.items()})\n",
        "    emb = outputs.logits.squeeze().cpu().numpy()\n",
        "    # normalize\n",
        "    if np.linalg.norm(emb) > 0:\n",
        "        emb = emb / np.linalg.norm(emb)\n",
        "    return emb\n",
        "\n",
        "# helper: extract audio from video file (raw wav path saved temporarily)\n",
        "def extract_audio_from_video(video_path, out_wav_path=\"/tmp/tmp_audio.wav\"):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    # prefer using ffmpeg if available ‚Äî but fallback to librosa via moviepy would be heavy.\n",
        "    # For reproducibility, we will try using librosa to load the video file directly (librosa can extract audio from many containers)\n",
        "    try:\n",
        "        y, sr = librosa.load(video_path, sr=AUDIO_SR)\n",
        "        import soundfile as sf\n",
        "        sf.write(out_wav_path, y, sr)\n",
        "        return out_wav_path\n",
        "    except Exception as e:\n",
        "        print(\"Audio extraction error for\", video_path, e)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36127667"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Build embeddings + labels (this may take time) ===\n",
        "import time, tqdm, os\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "all_visual_embs = []\n",
        "all_audio_embs = []\n",
        "labels = []\n",
        "paths = []\n",
        "\n",
        "for item in tqdm.tqdm(video_list):\n",
        "    vpath = item['path']\n",
        "    label = item['label']\n",
        "    paths.append(vpath)\n",
        "    # video -> sample frames -> face embeddings average\n",
        "    frames = sample_frames(vpath, n=SAMPLE_FRAMES)\n",
        "    face_embs = []\n",
        "    for f in frames:\n",
        "        fe = get_face_embedding(f)\n",
        "        if fe is not None:\n",
        "            face_embs.append(fe)\n",
        "    if len(face_embs) > 0:\n",
        "        visual_emb = np.mean(face_embs, axis=0)\n",
        "        if np.linalg.norm(visual_emb) > 0:\n",
        "            visual_emb = visual_emb / np.linalg.norm(visual_emb)\n",
        "    else:\n",
        "        visual_emb = np.zeros(512)\n",
        "    all_visual_embs.append(visual_emb)\n",
        "    # audio: extract audio from video and compute embedding\n",
        "    wav = extract_audio_from_video(vpath)\n",
        "    if wav is not None:\n",
        "        a_emb = extract_audio_embedding_from_file(wav)\n",
        "    else:\n",
        "        a_emb = np.zeros(768)\n",
        "    all_audio_embs.append(a_emb)\n",
        "    labels.append(label)\n",
        "\n",
        "all_visual_embs = np.vstack(all_visual_embs)\n",
        "all_audio_embs = np.vstack(all_audio_embs)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Embeddings shapes:\", all_visual_embs.shape, all_audio_embs.shape, labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f407211"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Cosine similarity examples (as in your original code) ===\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# compute average cosine similarity between audio and visual embeddings per class\n",
        "sim_per_sample = [cosine_similarity(all_audio_embs[i].reshape(1,-1), all_visual_embs[i].reshape(1,-1))[0,0] for i in range(len(labels))]\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'path': paths, 'label': labels, 'audio_visual_cosine': sim_per_sample})\n",
        "display(df.groupby('label').audio_visual_cosine.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23211223"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Train-test split for evaluation ===\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_audio_train, X_audio_test, X_vis_train, X_vis_test, y_train, y_test, paths_train, paths_test = train_test_split(\n",
        "    all_audio_embs, all_visual_embs, labels, paths, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "print(\"Train/Test sizes:\", X_audio_train.shape[0], X_audio_test.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2807591e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Simple fusion classifier training (MLP) on extracted embeddings ===\n",
        "import torch.nn as nn, torch.optim as optim, torch\n",
        "class SimpleFusionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_embs, vis_embs, labels):\n",
        "        self.a = torch.tensor(audio_embs).float()\n",
        "        self.v = torch.tensor(vis_embs).float()\n",
        "        self.y = torch.tensor(labels).long()\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.a[idx], self.v[idx], self.y[idx]\n",
        "\n",
        "train_ds = SimpleFusionDataset(X_audio_train, X_vis_train, y_train)\n",
        "test_ds = SimpleFusionDataset(X_audio_test, X_vis_test, y_test)\n",
        "\n",
        "fusion_model = nn.Sequential(\n",
        "    nn.Linear(X_audio_train.shape[1] + X_vis_train.shape[1], 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 2)\n",
        ").to(device)\n",
        "\n",
        "opt = optim.Adam(fusion_model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, ds, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    loader = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=True)\n",
        "    for a,v,y in loader:\n",
        "        a = a.to(device); v=v.to(device); y=y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(torch.cat((a,v), dim=1))\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def eval_model(model, ds):\n",
        "    model.eval()\n",
        "    loader = torch.utils.data.DataLoader(ds, batch_size=32, shuffle=False)\n",
        "    preds=[]; trues=[]\n",
        "    with torch.no_grad():\n",
        "        for a,v,y in loader:\n",
        "            a=a.to(device); v=v.to(device)\n",
        "            out = model(torch.cat((a,v), dim=1))\n",
        "            p = out.argmax(dim=1).cpu().numpy()\n",
        "            preds.extend(p.tolist()); trues.extend(y.numpy().tolist())\n",
        "    return preds, trues\n",
        "\n",
        "# train for a few epochs\n",
        "for epoch in range(8):\n",
        "    l = train_epoch(fusion_model, train_ds, opt)\n",
        "    preds_train, trues_train = eval_model(fusion_model, train_ds)\n",
        "    preds_test, trues_test = eval_model(fusion_model, test_ds)\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    print(f\"Epoch {epoch+1}: loss={l:.4f}, train_acc={accuracy_score(trues_train, preds_train):.4f}, test_acc={accuracy_score(trues_test, preds_test):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8afdd4c6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Confusion Matrix & Classification Report ===\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "preds_test, _ = eval_model(fusion_model, test_ds)  # preds_test, trues_test already computed in loop, but recompute for clarity\n",
        "y_true = y_test\n",
        "y_pred = preds_test\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=['Real','Fake']))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real','Fake'])\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "disp.plot(ax=ax)\n",
        "plt.title(\"Confusion Matrix (Fusion Model)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa9b2a4c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === t-SNE visualization of concatenated embeddings ===\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "emb_concat = np.concatenate([all_audio_embs, all_visual_embs], axis=1)\n",
        "# run PCA first to speed up TSNE if many samples\n",
        "if emb_concat.shape[0] > 200:\n",
        "    pca = PCA(n_components=50)\n",
        "    emb_pca = pca.fit_transform(emb_concat)\n",
        "else:\n",
        "    emb_pca = emb_concat\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "emb_2d = tsne.fit_transform(emb_pca)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for lab in np.unique(labels):\n",
        "    idx = labels==lab\n",
        "    plt.scatter(emb_2d[idx,0], emb_2d[idx,1], label=('Real' if lab==0 else 'Fake'), alpha=0.7)\n",
        "plt.legend()\n",
        "plt.title(\"t-SNE of concatenated audio+visual embeddings\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d002b4d"
      },
      "source": [
        "\n",
        "## üîÅ Wav2Vec2 Fine-tuning (Audio-only)\n",
        "\n",
        "This section fine-tunes a `Wav2Vec2ForSequenceClassification` model for binary classification on audio extracted from your videos. It uses Hugging Face `Trainer`.\n",
        "\n",
        "**Important:** Adjust `num_train_epochs` and `per_device_train_batch_size` according to your compute limits. Using GPU is highly recommended.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da452524"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Prepare an audio dataset for Trainer. This will process audio files and labels into HF dataset format.\n",
        "import torch, os\n",
        "from datasets import Dataset, Audio, DatasetDict, load_from_disk\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "# Build simple list of dicts for audio dataset: extract audio files first (re-using extract_audio_from_video)\n",
        "audio_examples = []\n",
        "for item in video_list:\n",
        "    wav = extract_audio_from_video(item['path'], out_wav_path=f\"/tmp/{Path(item['path']).stem}.wav\")\n",
        "    if wav is None:\n",
        "        continue\n",
        "    audio_examples.append({'audio': wav, 'label': item['label']})\n",
        "\n",
        "# create HF dataset\n",
        "hf_ds = Dataset.from_list(audio_examples)\n",
        "hf_ds = hf_ds.cast_column(\"audio\", Audio(sampling_rate=AUDIO_SR))\n",
        "hf_ds = hf_ds.train_test_split(test_size=0.2, stratify_by_column=\"label\", seed=42)\n",
        "\n",
        "def preprocess_function(batch):\n",
        "    audios = [x['array'] for x in batch['audio']]\n",
        "    inputs = feature_extractor(audios, sampling_rate=AUDIO_SR, padding=True, return_tensors='pt')\n",
        "    batch[\"input_values\"] = inputs['input_values'].numpy().tolist()\n",
        "    batch[\"attention_mask\"] = inputs['attention_mask'].numpy().tolist()\n",
        "    batch[\"labels\"] = batch[\"label\"]\n",
        "    return batch\n",
        "\n",
        "hf_ds = hf_ds.map(preprocess_function, batched=True, remove_columns=['audio'], batch_size=8)\n",
        "hf_ds = hf_ds.remove_columns([c for c in hf_ds['train'].column_names if c not in ('input_values','attention_mask','labels')])\n",
        "hf_ds.set_format(type='torch', columns=['input_values','attention_mask','labels'])\n",
        "\n",
        "print(hf_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "327aef65"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./wav2vec2_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=audio_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=feature_extractor,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting fine-tuning on Colab GPU...\")\n",
        "trainer.train()\n",
        "trainer.save_model(\"./wav2vec2_finetuned\")\n",
        "print(\"‚úÖ Fine-tuning completed and best model saved to ./wav2vec2_finetuned\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}